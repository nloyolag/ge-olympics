{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import sentiment as vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What was the impact of the GE brand in the collection of tweets gathered in the past Olympic games?\n",
    "\n",
    "tweets = []\n",
    "with open(\"filtered\") as f:\n",
    "    for line in f.readlines():\n",
    "        tweets.append(json.loads(line))\n",
    "        \n",
    "dataset = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create additional columns\n",
    "dataset['screen_name'] = dataset.user.map(lambda x: x['screen_name'].lower())\n",
    "dataset['name'] = dataset.user.map(lambda x: x['name'].lower())\n",
    "dataset['statuses_count'] = dataset.user.map(lambda x: x['statuses_count'])\n",
    "dataset['followers_count'] = dataset.user.map(lambda x: x['followers_count'])\n",
    "dataset['text_lower'] = dataset.text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ge_accounts = ['generalelectric', \n",
    "               'ge_reports', \n",
    "               'gedobrasil', \n",
    "               'ge_canada', \n",
    "               'gehealthcare', \n",
    "               'ge_digital', \n",
    "               'gedesign', \n",
    "               'ge_water', \n",
    "               'gecareeers_latam', \n",
    "               'geaviation', \n",
    "               'ge_appliances', \n",
    "               'gepublicaffairs', \n",
    "               'gecapital', \n",
    "               'ge_uk', \n",
    "               'geinfosec', \n",
    "               'ge_power', \n",
    "               'geresearch', \n",
    "               'ge_oilandgas', \n",
    "               'gelighting', \n",
    "               'ge_foundation', \n",
    "               'ge_gaspower',\n",
    "               'ge_grid']\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accounts': {u'ge_canada': {'followers': 16560,\n",
       "   'name': u'ge canada',\n",
       "   'tweets': 4016},\n",
       "  u'ge_gaspower': {'followers': 2977,\n",
       "   'name': u'gas power systems',\n",
       "   'tweets': 2936},\n",
       "  u'ge_grid': {'followers': 4115,\n",
       "   'name': u'ge grid solutions',\n",
       "   'tweets': 2457},\n",
       "  u'ge_oilandgas': {'followers': 41246,\n",
       "   'name': u'ge oil & gas',\n",
       "   'tweets': 3352},\n",
       "  u'ge_power': {'followers': 14426, 'name': u'ge power', 'tweets': 4308},\n",
       "  u'ge_reports': {'followers': 56046, 'name': u'ge reports', 'tweets': 4863},\n",
       "  u'ge_uk': {'followers': 2636, 'name': u'ge uk', 'tweets': 1311},\n",
       "  u'geaviation': {'followers': 146388,\n",
       "   'name': u'ge aviation',\n",
       "   'tweets': 10841},\n",
       "  u'gehealthcare': {'followers': 78397,\n",
       "   'name': u'ge healthcare',\n",
       "   'tweets': 18407},\n",
       "  u'gepublicaffairs': {'followers': 9564,\n",
       "   'name': u'ge public affairs',\n",
       "   'tweets': 11221}},\n",
       " 'hashtags': {'#ge': 26,\n",
       "  '#ge_appliances': 0,\n",
       "  '#ge_canada': 0,\n",
       "  '#ge_digital': 0,\n",
       "  '#ge_foundation': 0,\n",
       "  '#ge_gaspower': 0,\n",
       "  '#ge_grid': 0,\n",
       "  '#ge_oilandgas': 0,\n",
       "  '#ge_power': 0,\n",
       "  '#ge_reports': 0,\n",
       "  '#ge_uk': 0,\n",
       "  '#ge_water': 0,\n",
       "  '#geaviation': 0,\n",
       "  '#gecapital': 0,\n",
       "  '#gecareeers_latam': 0,\n",
       "  '#gedesign': 0,\n",
       "  '#gedobrasil': 0,\n",
       "  '#gehealthcare': 0,\n",
       "  '#geinfosec': 0,\n",
       "  '#gelighting': 1,\n",
       "  '#general_electric': 0,\n",
       "  '#generalelectric': 1,\n",
       "  '#gepublicaffairs': 0,\n",
       "  '#geresearch': 0,\n",
       "  '$ge': 7},\n",
       " 'mentions': {'@ge': 0,\n",
       "  '@ge_appliances': 0,\n",
       "  '@ge_canada': 1,\n",
       "  '@ge_digital': 1,\n",
       "  '@ge_foundation': 0,\n",
       "  '@ge_gaspower': 0,\n",
       "  '@ge_grid': 9,\n",
       "  '@ge_oilandgas': 2,\n",
       "  '@ge_power': 22,\n",
       "  '@ge_reports': 1,\n",
       "  '@ge_uk': 4,\n",
       "  '@ge_water': 0,\n",
       "  '@geaviation': 27,\n",
       "  '@gecapital': 0,\n",
       "  '@gecareeers_latam': 0,\n",
       "  '@gedesign': 0,\n",
       "  '@gedobrasil': 0,\n",
       "  '@gehealthcare': 88,\n",
       "  '@geinfosec': 0,\n",
       "  '@gelighting': 0,\n",
       "  '@generalelectric': 132,\n",
       "  '@gepublicaffairs': 3,\n",
       "  '@geresearch': 0},\n",
       " 'text_mentions': {'ge.com': 0,\n",
       "  'general electric': 14,\n",
       "  'generalelectric': 133},\n",
       " 'total_hashtags': 26,\n",
       " 'total_mentions': 290,\n",
       " 'total_text_mentions': 147,\n",
       " 'tweets-by-ge': 74}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Volume\n",
    "volume_dict = {}\n",
    "\n",
    "# Tweets by GE\n",
    "ge_tweets = dataset[dataset['screen_name'].isin(ge_accounts)]\n",
    "volume_dict[\"tweets-by-ge\"] = len(ge_tweets)\n",
    "\n",
    "# Followers and tweet count per account\n",
    "volume_dict['accounts'] = {}\n",
    "ge_tweets_unique = ge_tweets.drop_duplicates(subset='screen_name')\n",
    "ge_tweets_unique = ge_tweets_unique[['name', 'screen_name', 'statuses_count', 'followers_count']]\n",
    "for index, row in ge_tweets_unique.iterrows():\n",
    "    volume_dict['accounts'][row['screen_name']] = {\n",
    "        'name': row['name'],\n",
    "        'tweets': row['statuses_count'],\n",
    "        'followers': row['followers_count'] \n",
    "    }\n",
    "         \n",
    "# Function to get counts per element\n",
    "def get_counts(elem_list, volume_dict, dataset, key):\n",
    "    current_count = 0\n",
    "    total_count = 0\n",
    "    for elem in elem_list:\n",
    "        current_count = len(dataset[dataset['text_lower'].str.contains(elem)])\n",
    "        volume_dict[key][elem] = current_count\n",
    "        total_count += current_count\n",
    "    return total_count\n",
    "\n",
    "# Function to get counts for a regex\n",
    "def get_regex_counts(volume_dict, dataset, key, regex, regexstr):\n",
    "    count = len(dataset[dataset['text_lower'].str.match(regex)])\n",
    "    volume_dict[key][regexstr] = count\n",
    "    return count\n",
    "\n",
    "# Dataset excluding GE's accounts\n",
    "exclude_ge = dataset[~dataset['screen_name'].isin(ge_accounts)]\n",
    "    \n",
    "# Mentions of GE accounts\n",
    "volume_dict['mentions'] = {}\n",
    "ge_mentions = list(ge_accounts)\n",
    "ge_mentions = ['@{0}'.format(account) for account in ge_mentions]\n",
    "ge_regex = re.compile('@ge | @ge$')\n",
    "total_mentions = get_counts(ge_mentions, volume_dict, exclude_ge, 'mentions')\n",
    "total_mentions += get_regex_counts(volume_dict, exclude_ge, 'mentions', ge_regex, \"@ge\")\n",
    "volume_dict['total_mentions'] = total_mentions\n",
    "\n",
    "# Hashtags of GE\n",
    "volume_dict['hashtags'] = {}\n",
    "ge_hashtags = list(ge_accounts)\n",
    "ge_hashtags = ['#{0}'.format(account) for account in ge_hashtags]\n",
    "ge_hashtags.append(\"#general_electric\")\n",
    "ge_hashtag_regex = re.compile('#ge | #ge$')\n",
    "ge_dollar_regex = re.compile('\\$ge | \\$ge$')\n",
    "total_hashtags = get_counts(ge_hashtags, volume_dict, exclude_ge, 'hashtags')\n",
    "total_hashtags = get_regex_counts(volume_dict, exclude_ge, 'hashtags', ge_dollar_regex, \"$ge\")\n",
    "total_hashtags = get_regex_counts(volume_dict, exclude_ge, 'hashtags', ge_hashtag_regex, \"#ge\")\n",
    "volume_dict['total_hashtags'] = total_hashtags\n",
    "\n",
    "# GE mentioned in text\n",
    "volume_dict['text_mentions'] = {}\n",
    "ge_text = ['general electric', 'generalelectric']\n",
    "ge_website_regex = re.compile('ge.com | ge.com$')\n",
    "total_hashtags = get_counts(ge_text, volume_dict, exclude_ge, 'text_mentions')\n",
    "total_hashtags += get_regex_counts(volume_dict, exclude_ge, 'text_mentions', ge_website_regex, \"ge.com\")\n",
    "volume_dict['total_text_mentions'] = total_hashtags\n",
    "\n",
    "volume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1d3815af75d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# All tweets made by GE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mge_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mge_accounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mge_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'followers_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mge_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'followers_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mge_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mge_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retweet_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'followers_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'favorite_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Reach\n",
    "\n",
    "# All tweets made by GE\n",
    "ge_tweets = dataset[dataset['screen_name'].isin(ge_accounts)]\n",
    "ge_tweets['followers_count'] = ge_tweets.user.map(lambda x: x['followers_count'])\n",
    "ge_tweets = ge_tweets[['id', 'screen_name', 'retweet_count', 'followers_count', 'favorite_count']]\n",
    "\n",
    "# GE retweeters\n",
    "exclude_ge = dataset[~dataset['screen_name'].isin(ge_accounts)]\n",
    "ge_retweets = ['^rt @{0}'.format(account) for account in ge_accounts]\n",
    "ge_retweeters = dataset[dataset['text_lower'].str.contains('|'.join(ge_retweets))]\n",
    "ge_retweeters = ge_retweeters[~ge_retweeters['retweeted_status'].isnull()]\n",
    "ge_retweeters['original_tweet_id'] = ge_retweeters.retweeted_status.map(lambda x: x['id'])\n",
    "ge_retweeters['followers_count'] = ge_retweeters.user.map(lambda x: x['followers_count'])\n",
    "ge_retweeters = ge_retweeters[['original_tweet_id', 'followers_count', 'favorite_count']]\n",
    "\n",
    "# Calculate reach by summing followers of each retweeter\n",
    "retweet_reach = ge_retweeters.groupby('original_tweet_id')\n",
    "retweet_reach = retweet_reach.aggregate([np.sum])\n",
    "\n",
    "# Join the reach results to the original tweets\n",
    "reach = pd.merge(ge_tweets, retweet_reach, left_on='id', right_index=True, how='left')\n",
    "reach.columns = ['id', \n",
    "                 'screen_name', \n",
    "                 'retweet_count', \n",
    "                 'followers_count', \n",
    "                 'favorite_count', \n",
    "                 'retweet_followers',\n",
    "                 'retweet_favorites',\n",
    "                ]\n",
    "reach = reach.fillna(0)\n",
    "reach['total_favorites'] = reach['retweet_favorites'] + reach['favorite_count']\n",
    "reach['total_impressions'] = reach['retweet_followers'] + reach['followers_count']\n",
    "\n",
    "# Group results by account\n",
    "reach_per_account = reach.drop('id', 1)\n",
    "reach_per_account = reach.groupby('screen_name')\n",
    "reach_per_account = reach_per_account.aggregate([np.sum, np.mean, np.std])\n",
    "reach_per_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentiment\n",
    "corpus = dataset['text_lower']\n",
    "sentiments = {}\n",
    "\n",
    "def get_sentiment(scores):\n",
    "    positive_value = scores['pos']\n",
    "    negative_value = scores['neg']\n",
    "    neutral_value = scores['neu']\n",
    "    \n",
    "    values = [positive_value, negative_value, neutral_value]\n",
    "    \n",
    "    if positive_value == max(values): return \"positive\"\n",
    "    if negative_value == max(values): return \"negative\"\n",
    "    if neutral_value == max(values): \n",
    "        if positive_value > 0.25 or negative_value > 0.25:\n",
    "            return \"positive\" if positive_value > negative_value else \"negative\"\n",
    "    \n",
    "def analyse_sentiment(corpus):\n",
    "    for index, row in corpus.iteritems():\n",
    "        scores = vaderSentiment(row.encode('utf-8'))\n",
    "        sentiments[row] = get_sentiment(scores)\n",
    "        \n",
    "analyse_sentiment(corpus)\n",
    "\n",
    "positive_tweets = sum(1 for x in sentiments.values() if x == \"positive\")\n",
    "negative_tweets = sum(1 for x in sentiments.values() if x == \"negative\")\n",
    "\n",
    "#View tweets (negative)\n",
    "#{k: v for k, v in sentiments.iteritems() if v == \"negative\"}\n",
    "\n",
    "#...as percentages\n",
    "#positive_tweets / float(len(corpus))\n",
    "#negative_tweets / float(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>emiforlove</th>\n",
       "      <td>476925</td>\n",
       "      <td>1</td>\n",
       "      <td>476925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prweekuknews</th>\n",
       "      <td>63902</td>\n",
       "      <td>2</td>\n",
       "      <td>127804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admitonesin</th>\n",
       "      <td>114538</td>\n",
       "      <td>1</td>\n",
       "      <td>114538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zain_verjee</th>\n",
       "      <td>67259</td>\n",
       "      <td>1</td>\n",
       "      <td>67259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gateway978</th>\n",
       "      <td>55109</td>\n",
       "      <td>1</td>\n",
       "      <td>55109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbdony</th>\n",
       "      <td>18962</td>\n",
       "      <td>2</td>\n",
       "      <td>37924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>findsfromyester</th>\n",
       "      <td>37643</td>\n",
       "      <td>1</td>\n",
       "      <td>37643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sportsfeatures</th>\n",
       "      <td>34532</td>\n",
       "      <td>1</td>\n",
       "      <td>34532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atsocialmediauk</th>\n",
       "      <td>33856</td>\n",
       "      <td>1</td>\n",
       "      <td>33856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cfm_engines</th>\n",
       "      <td>32736</td>\n",
       "      <td>1</td>\n",
       "      <td>32736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lkafle</th>\n",
       "      <td>31583</td>\n",
       "      <td>1</td>\n",
       "      <td>31583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lindaleeking</th>\n",
       "      <td>26751</td>\n",
       "      <td>1</td>\n",
       "      <td>26751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ge_grid</th>\n",
       "      <td>4303</td>\n",
       "      <td>6</td>\n",
       "      <td>25818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderopolis</th>\n",
       "      <td>25253</td>\n",
       "      <td>1</td>\n",
       "      <td>25253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sheffieldis</th>\n",
       "      <td>19932</td>\n",
       "      <td>1</td>\n",
       "      <td>19932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allnurses</th>\n",
       "      <td>19390</td>\n",
       "      <td>1</td>\n",
       "      <td>19390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sporttechie</th>\n",
       "      <td>17023</td>\n",
       "      <td>1</td>\n",
       "      <td>17023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jimmyhoshi</th>\n",
       "      <td>7567</td>\n",
       "      <td>2</td>\n",
       "      <td>15134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1dphil_ad</th>\n",
       "      <td>13108</td>\n",
       "      <td>1</td>\n",
       "      <td>13108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttamaremo</th>\n",
       "      <td>4249</td>\n",
       "      <td>3</td>\n",
       "      <td>12747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embracespectrum</th>\n",
       "      <td>2899</td>\n",
       "      <td>4</td>\n",
       "      <td>11596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qrcodeid1</th>\n",
       "      <td>11379</td>\n",
       "      <td>1</td>\n",
       "      <td>11379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leo_rommel</th>\n",
       "      <td>11359</td>\n",
       "      <td>1</td>\n",
       "      <td>11359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autismnewsnow</th>\n",
       "      <td>11305</td>\n",
       "      <td>1</td>\n",
       "      <td>11305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geautomation</th>\n",
       "      <td>8979</td>\n",
       "      <td>1</td>\n",
       "      <td>8979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tagthink</th>\n",
       "      <td>8937</td>\n",
       "      <td>1</td>\n",
       "      <td>8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>giantstepsil</th>\n",
       "      <td>8246</td>\n",
       "      <td>1</td>\n",
       "      <td>8246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dabbourbahjat</th>\n",
       "      <td>8127</td>\n",
       "      <td>1</td>\n",
       "      <td>8127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorcycle_guy</th>\n",
       "      <td>7757</td>\n",
       "      <td>1</td>\n",
       "      <td>7757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortraders</th>\n",
       "      <td>7373</td>\n",
       "      <td>1</td>\n",
       "      <td>7373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patricialahozm</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danboilertech</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jerseyfunellie</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quentinbauden</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clem1877</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andre_obst</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denimsureogn</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davidgrainger</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maryjelks</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gatiex_2787</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>globalshipassoc</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ferhatucar_ee</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rosina_larios</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flrehawkcam</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>archaicsmileinc</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rio16news1</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mgargat</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_colasanto</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emmylouuk</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iiovericegum</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webteamclt</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crazyberds</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fjason0306</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthcarenws</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scottwgardner1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>einsightshealth</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geafricainnov8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadjer_menni</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puellaexanglia</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>njbabc</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 followers_count  tweet_count  impressions\n",
       "screen_name                                               \n",
       "emiforlove                476925            1       476925\n",
       "prweekuknews               63902            2       127804\n",
       "admitonesin               114538            1       114538\n",
       "zain_verjee                67259            1        67259\n",
       "gateway978                 55109            1        55109\n",
       "bbdony                     18962            2        37924\n",
       "findsfromyester            37643            1        37643\n",
       "sportsfeatures             34532            1        34532\n",
       "atsocialmediauk            33856            1        33856\n",
       "cfm_engines                32736            1        32736\n",
       "lkafle                     31583            1        31583\n",
       "lindaleeking               26751            1        26751\n",
       "ge_grid                     4303            6        25818\n",
       "wonderopolis               25253            1        25253\n",
       "sheffieldis                19932            1        19932\n",
       "allnurses                  19390            1        19390\n",
       "sporttechie                17023            1        17023\n",
       "jimmyhoshi                  7567            2        15134\n",
       "1dphil_ad                  13108            1        13108\n",
       "ttamaremo                   4249            3        12747\n",
       "embracespectrum             2899            4        11596\n",
       "qrcodeid1                  11379            1        11379\n",
       "leo_rommel                 11359            1        11359\n",
       "autismnewsnow              11305            1        11305\n",
       "geautomation                8979            1         8979\n",
       "tagthink                    8937            1         8937\n",
       "giantstepsil                8246            1         8246\n",
       "dabbourbahjat               8127            1         8127\n",
       "motorcycle_guy              7757            1         7757\n",
       "fortraders                  7373            1         7373\n",
       "...                          ...          ...          ...\n",
       "patricialahozm                25            1           25\n",
       "danboilertech                  8            3           24\n",
       "jerseyfunellie                24            1           24\n",
       "quentinbauden                 24            1           24\n",
       "clem1877                      23            1           23\n",
       "andre_obst                    20            1           20\n",
       "denimsureogn                  19            1           19\n",
       "davidgrainger                 19            1           19\n",
       "maryjelks                     18            1           18\n",
       "gatiex_2787                   18            1           18\n",
       "globalshipassoc               16            1           16\n",
       "ferhatucar_ee                 16            1           16\n",
       "rosina_larios                 15            1           15\n",
       "flrehawkcam                   15            1           15\n",
       "archaicsmileinc               15            1           15\n",
       "rio16news1                    12            1           12\n",
       "mgargat                       11            1           11\n",
       "e_colasanto                    8            1            8\n",
       "emmylouuk                      7            1            7\n",
       "iiovericegum                   6            1            6\n",
       "webteamclt                     5            1            5\n",
       "crazyberds                     4            1            4\n",
       "fjason0306                     4            1            4\n",
       "healthcarenws                  3            1            3\n",
       "scottwgardner1                 3            1            3\n",
       "einsightshealth                3            1            3\n",
       "geafricainnov8                 3            1            3\n",
       "hadjer_menni                   2            1            2\n",
       "puellaexanglia                 1            1            1\n",
       "njbabc                         0            1            0\n",
       "\n",
       "[399 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Audience Characteristics\n",
    "\n",
    "# Lang\n",
    "# Location\n",
    "\n",
    "# Most active participants\n",
    "top_participants = exclude_ge[['screen_name', 'text_lower']].groupby('screen_name').agg('count').rename(columns={'text_lower': 'tweet_count'})\n",
    "\n",
    "# Most relevant participants\n",
    "relevant_participants = exclude_ge[['screen_name', 'followers_count']].groupby('screen_name').agg('max')\n",
    "relevant_participants = pd.merge(relevant_participants, top_participants, left_index=True, right_index=True, how='left')\n",
    "relevant_participants['impressions'] = relevant_participants['followers_count'] * relevant_participants['tweet_count']\n",
    "relevant_participants = relevant_participants.sort_values(by='impressions', ascending=False)\n",
    "relevant_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
